\section{Deep Batch RL (for Education)}
\label{deepRL}

In this project, we want to compare two methods, the model-free method (DRQN) and the model-based method (expectimax search with DKT), both based on deep learning models, on off-policy learning for offline (i.e. a batch of) educational data.

\subsection{Model-based and Model-free RL}

In RL, our goal is to find the optimal policy in an unknown environment. There are mainly three ways of doing RL (the latter two are model-free methods, while we only focus on the comparison of the first two):

\begin{itemize}
\item First explicitly learn the model for the environment, i.e., the reward and transition model, then find the optimal policy using MDP (Markov Decision Process) method based on those.
\item Learn the Q values, i.e. the expected long-term utility of doing action $a$ in state $s$, $Q(s, a)$, without learning the environment model. With Q values, we can easily derive the optimal policy: $\pi(s)=\argmax\limits_a Q(s, a)$.
\item Directly learn the policy, i.e., we are directly mapping a state $s$ to an action $a$, without explicitly computing any model or utility. 
\end{itemize}

Our hypothesis is, model-free method should hold an advantage over model-based method in various aspects. First, a more apparrent one, model-free method is more computational feasible to scale-up, while doing MDP with a world model might involve tree searches, which is exponential. The more relevant advantage of a model-free method is that it's more sample efficient: you need a lot of samples to train an accurate world model (a slightly inaccurate model might suffer from an error explosion in the decision making process, thus result in a bad policy), but way less sample to simply know which action is better given the current state (i.e., no need to know the exact long-term utility).

\subsection{RL Going Deep}

\subsubsection{Deep World Model}

DKT is a good world model in education. It's a simple step forward to adopt DKT to do planning. With a DKT, we know the knowledge belief state for the student given any practice history, so we can evaluate any policy based on this.

To be more specific, we do a expectimax search with DKT. An expectimax seach tree has interleaving layers of max and expect nodes. A max node pick the children with the maximum value, while an expect node's value is an expectation of the value of its children. In our case, a max node will have children expect nodes each corresponding to an action (i.e. which skill to teach). An expect node will have children max nodes each corresponding to an 

\subsubsection{Deep Q Learning}

We are doing a Q-Learning algorithm in our project. Q-Learning is an off-policy algorithm, which means that the policy we pick in updating/learning Q-values isn't necessarily the actual policy we run. The Q-value updating function is (for discrete states):

\begin{equation*}
    Q(s,a)= (1-\alpha)Q(s,a) + \alpha(R+\gamma\max\limits_{a'}Q(s',a'))
\end{equation*}

where $\alpha$ is a constant, like a learning rate. From this equation we can get a better understanding of off-policy: the $a'$ we pick is not the action that actually run, but the action than can maximize the Q value in state $s'$.

Our project is closely related to the Deep Q-Network (DQN) from Google Deepmind. DQN is an online (but off-policy) algorithm, which does exploration using $\epsilon-greedy$ search during training. However, when training the network, DQN employs an experience replay mechanism which feed randomized $(s, a, r, s')$ experience minibatches to CNN, which is pretty much off-line training. \reminder{Emma confirms that the online part, i.e. the $\epsilon-greedy$ search part does matter, but didn't explain in detail.} However, at least we know that Deep Q-Learning is totally feasible when changed to off-line.

First, for real world data, DKT (trained on a hold out dataset, unseen by DRQN or the expectimax-DKT) should serve as the "ground truth". This will be a comparison of the gener- alization ability of both methods. We are hoping that DRQN is better, since it’s more sample efficient (you need a lot of samples to train an accurate world model, but way less sample to simply know which is better). But a model-based method has its own intrinstic advantage, and there is less "black magic" than in DRQN. [** This seems to be a reinforment-learning related method that I should read more about. It’s one of the papers studied in the group meetings. **]

Expectimax is intractable when we have more action choices, aka skills. On simulated data, we have the perfect performance, which is expectimax-BKT. We are hoping to see that DRQN can perform on par with this, and DKT should be worse (since DKT is only an estimation to the world model).


I had some dicussions with Li on several understandings of DRQN (although I’m not doing any experiments on that). Considering the two major tricks proposed in the DQN paper, replay only targets at online learning (where there are always new data coming, so we might want to sample mini-batches from the pool of our exploration history). Since we are doing epsilon- greedy exploration, the distribution of data acquired during training is actually shifting, thus very unstable. Therefore, we need to have a target network as the second trick, which updates slower and provides a more stable training target.

Another subtlety considering Q-learning is that it requires each update to be independant, which is another reason for doing replay (breaking up the trajectories). In the DRQN paper, they reported that feeding full sequences of trajectories (thus updates are dependant) or ran- dom segments of trajectories (thus independant updates) doesn’t affect the performance. An- other magic of neural networks.

\subsection{Batch RL's Constraints}

In RL, when we talk about batch we mean offline (so we have a huge batch of fixed data to train on). For online RL, it’s single data entry (since we are constantly getting new data through greedy dearch) or mini-batch. We are doing an offline RL task, thus batch RL.

Reward design

For model-free methods, we need to design a reward function that can get the rewards directly out of data. We are not looking for a myopic definition like getting a question correct. Currently we are using a one-time-only reward associated to each skill, which is awarded once the student get 3 corrects in a row. [** If we re-coded the joint skill questions, we need to unpack the skills and count for each skill separately. **]
[** Here are more thoughts on multi-skill exercises. It matters in accounting rewards during training. Should we giving multi-skill questions during planning as well? What kind of combination of skills should we give? Is there a way to decide what is a "powerful" skill-combo? **]
There are more variants of reward functions. A more natural one is to give a decaying re- ward for the same skill question. If we taking the applicability into consideration, we probably should also give negative rewards for the same skill since we could run out of exercises to give.
To make the training of DRQN more stable, we could also try giving awards only when trial ends. Some regularization may be needed. [** I’m not very clear with the details in this part. **]
And we could also use a discounted rewards, since we want to get rewards in the near future (the sooner the better).

Exploration

However, for offline training, the data distribution is fixed, so maybe we don’t need a target network. However, emperically, it doesn’t make a difference. The loss would still explode when trained with many epochs. It needs mentioning that when the loss is at its minimum, the planning performance IS at its best.

One interesting finding (a sanity check actually) is that, when we modify the algorithm to online (since we have the ground truth simulator) it can perform on par with the optimal policy. When we do greedy search, it converges fast (because we have a lot of near-optimal sequences), while when we do completely random search, the performance drops after a short boost (just like the offline performance), then come back to the optimal performance eventually, which could be mainly due to the massive random dataset (in which there are enough good/near- optimal examples) that has already added in the pool. Greedy search is basically changing the training dataset, so we can easily accept the fact that it’s way better.
Our main goal is to compare model-based and model-free methods on doing tutor plan- ning. DRQN and DKT, although both adopt an LSTM as core structure, are very different function approximation methods (not even close). The current drawback of DRQN is clear: unable to do exploration, thus require a massive amount (5 ∗ 105 episodes) of random data to see enough near-optimal trajectories. On the contrary, DKT can easily pick up a vague pre- requisite structure even under a small dataset (100episodes), by observing that some skills are easier to adopt in the begining while others are not. Based on this little knowledge, the expec- timax can get us an optimal policy. In this current setting, model-based method seems to win (especially when model-free method is not that sample efficient as we thought it should be).
There are several concerns on the current experimant setting. We only has 20 trials for each episode, i.e. our horizon is fairly short. A method as shallow as DKT-expectimax (5 steps look ahead) can perform as near-perfect because we can only expect the students to learn those skills without prerequisites. If we have a setting where a shallow sight might be catastrphic (like a
31
longer horizon and a more complicated skill structure), we might see DRQN to win, since it can look infinite steps.

\subsection{Future Work}

There is a very important thing to keep in mind: if all skills are independant, we don’t need to do expectimax, since it doesn’t matter what question we give (order doesn’t matter in this case). In another word, the more complicated the relationships between skills are, the more
22
steps we should look-ahead in the tree search. Therefore, we probably don’t have to look too far away (a.k.a. having a long horizon) in our simple BKT simulation. We probably need to do more experiments to see if a long horizon can give us a lot of benefits.
[** Random thoughts: is there a relationship between the optimal length of horizon and complexity of the constraints? By optimal, I mean the length is long enough to capture the constraints, but still short enough to compute quickly. There are two obvious extreme cases: when there is no constraints we don’t need to search at all, and when we can look infinite steps we are guarantteed to get the optimal result no matter how hard it is. **]
Considering the horizon, we should also consider that the students might dropout so the horizon could be shorter than we searched.
For the speed problem, since we are still not sure if it’s necessary to have a long horizon, we can put aside the running speed issue a bit. However, the DKT can be accelerated by stacking the actions together and have a huge matrix computation instead of many small ones.
We should first focus on other more important part of the planning. First, we should be able to keep track to the sequence to allow the computation of model-free rewards (which could have various ways, and will be discussed in the next section). Moreover, we need to print the policy and see if the planning result makes sense (probably we can make a beautiful visualization).


Real Model Based

Accurate BKT.