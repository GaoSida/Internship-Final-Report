\section{Deep Batch RL (for Education)}
\label{deepRL}

In this project, we want to compare two methods, the model-free method (DRQN) and the model-based method (expectimax search with DKT), both based on deep learning models, on off-policy learning for offline (i.e. a batch of) educational data. I focused on the model-based method, including its implementation and evaluation. The model-free method is done by Li Zhou, a graduate student in our group, and I was involved in the discussions of it.

\subsection{Model-based and Model-free RL}
\label{subsec:model-free}

In RL, our goal is to find the optimal policy in an unknown environment. There are mainly three ways of doing RL (the latter two are model-free methods, while we only focus on the comparison of the first two):

\begin{itemize}
\item First explicitly learn the model for the environment, i.e., the reward and transition model, then find the optimal policy using MDP (Markov Decision Process) method based on those.
\item Learn the Q values, i.e. the expected long-term utility of doing action $a$ in state $s$, $Q(s, a)$, without learning the environment model. With Q values, we can easily derive the optimal policy: $\pi(s)=\argmax\limits_a Q(s, a)$.
\item Directly learn the policy, i.e., we are directly mapping a state $s$ to an action $a$, without explicitly computing any model or utility. 
\end{itemize}

Our hypothesis is, model-free method should hold an advantage over model-based method in various aspects. First, a more apparrent one, model-free method is more computational feasible to scale-up, while doing MDP with a world model might involve tree searches, which is exponential. The more relevant advantage of a model-free method is that it's more sample efficient: you need a lot of samples to train an accurate world model (a slightly inaccurate model might suffer from an error explosion in the decision making process, thus result in a bad policy), but way less sample to simply know which action is better given the current state (i.e., no need to know the exact long-term utility).

\subsection{RL Going Deep}

\subsubsection{Deep World Model}

DKT is a good world model in education. It's a simple step forward to adopt DKT to do planning. With a DKT, we know the knowledge belief state for the student given any practice history, so we can evaluate any policy based on this.

To be more specific, we do a expectimax search with DKT. An expectimax seach tree has interleaving layers of max and expect nodes. A max node pick the children with the maximum value, while an expect node's value is an expectation of the value of its children. In our case, a max node will have children expect nodes each corresponding to an action (i.e. which skill to teach). An expect node will have children max nodes each corresponding to a possible probabilty for each outcome of the action (i.e., correct or incorrect). DKT can assign a probabilty to each outcome based on the observed history, thus an expectation can be computed over the children max nodes.

Unlike other tree search like minmax, expectimax tree could not be trimmed without a loss on the accuracy (since it's computing an expectation, and all the branch values are needed). Therefore, expectimax is expensive to compute (as is the case for most model-based algorithms).

Besides conventional speed-up coding tricks like minimize the function calls and the use of abstract classes, we specifically made an optimization for DKT. Since DKT does sveral matrix multiplications for each input vector, we can stack several vectors into an input matrix and compute the output simultaneously. This trick significanly reduces the computation time to the one sixth of the unstacked version.

\subsubsection{Deep Q-Learning}

Q-Learning, a model-free RL algorithm, is off-policy, which means that the policy we pick in updating and learning Q-values isn't necessarily the actual policy we run. The Q-values should satisfy equation \ref{eq:Q}, the Bellman function.

\begin{equation}
\label{eq:Q}
    Q(s,a)= R+\gamma\max\limits_{a'}Q(s',a'))
\end{equation}

where $R$ is the reward of state $s$, $\gamma$ is a discount factor for future's rewards, and other notations consistent with the description in \ref{subsec:model-free}. From this equation we can get a better understanding of off-policy: the $a'$ we pick is not the action that actually run, but the action than can maximize the Q value in state $s'$.

For discrete states we can simply put the Q values in a table and do value iterations until the Bellman functions are satisfied (close to iterative methods to solve linear equations). Yet we can't do that for continuous states as in our case, so we want to learn a parameterized function to approximate the Q function. Previous works have shown that this function approximation is not guaranteed to converge when it's not linear, especially when it's a neural network.

The Deep Q-Network\footnote{\textit{Human-level control through deep reinforcement learning}, Google DeepMind, 2016.} (DQN) adopts several ticks to resolve this diverging issue. DQN is an online (but still off-policy) algorithm, which does online exploration using $\epsilon-greedy$ search during training. Q-learning requires each update to be independant, thus when training the network, DQN employs an experience replay mechanism which feed randomized $(s, a, r, s')$ experience minibatches sampled from the pool of exploration history to the neural network. The loss that the neural network is minimizing is the difference of the left-hand and the right-hand size of equation \ref{eq:Q}.

The second trick is having a target network, which is identical to DQN but updated with a lower frequency. Since we are doing $\epsilon-greedy$ exploration, the distribution of data acquired during training is actually shifting, thus making the training very unstable (both sides of the Bellman equation is changing). If the two sides of the Bellman equation is provided by the same DQN, it would be like a dog chasing its own tail. The target network, however, can serve as a stable right-hand side of equation \ref{eq:Q}, thus resolve the diverging issue.

\subsubsection{Deep Recurrent Q-Learning}

The DQN deals with Atari games, where the state is fully observable from the consecutive 4 frames of the game. However, in our case, we are dealing with a partially observable environment, since we can only observe whether a student gets a problem correct, rather than their actual knowledge status. Therefore, we need to adopt an LSTM to map an action history (observations) to the Q-value. Previous work \footnote{\textit{Deep Recurrent Q-Learning for Partially Observable MDPs}, Matthew Hausknecht and Peter Stone, 2015.} has already added this extention to DQN, resulting in Deep Recurrent Q-Network (DRQN). However, in their work, the training of DRQN is still an online process.

Although DKT and DRQN both adopt an LSTM, both take the trajectory of students' actions as inputs, and even have the same number of outputs (one for each skill), they are very different function approximators (i.e. completely different loss functions). DKT's output is an approximate of the student's knowledge status, i.e. the probability of getting each skill correct, thus a supervised learning model (the result of the next question serves as the label). The output of DRQN is the long-term utility (i.e. rewards) of picking each skill as next action (i.e. question to give). Unlike DKT, the DRQN can be directly used to pick a policy. 

For real world data, besides being a comparison model of DRQN, another DKT instance, which is trained on a hold-out dataset unseen by DRQN or the expectimax-DKT, should serve as the "ground truth" for the evaluation of the policy picked by both DRQN and expectimax-DKT. However, we start with a comparison on BKT simualtion data, where the ground truth BKT model is known, thus a fair comparison between DRQN and expectimax-DKT.

\subsection{Challenges in Batch RL}

\subsubsection{Reward Function Design}

In any RL practice, we'll always face the challenge of how to properly define a reward function, since in most cases, the rewards would not be explicitly presented in ths data. A myopic or misspecified reward function might result in very undesirable outcomes\footnote{\textit{Concrete Problems in AI Safety}, Dario Amodei, Chris Olah, John Schulman, Jacob Stein- hardt, Paul Christiano, Dan Mane, 2016.}.

The reward function should be based on the trajectories in the data. We are not looking for myopic definitions like getting a reward whenever question is correctly answered (which would result in the agent constanly giving the same question to hack a high reward). Currently we are using a one-time-only reward associated to each skill, which is awarded once the student get 3 correct in a row.

In our DKT-expectimax framework, any trajectory based reward function can be swapped in. There are more variants of reward functions. A more natural one is to give a decaying reward for questions on the same skill. If we take the applicability into consideration, we probably should also give negative rewards for too many exercises on the same skill since we could run out of exercises in our pool. To make the training of DRQN more stable, we could also try giving awards only when trial ends, where some regularization may be needed. And we could also use discounted rewards, since we want to get rewards in the near future (the sooner the better).

\subsubsection{Constraints on Exploration}

In RL, when we talk about batch we mean offline (since we have a huge batch of fixed data to train on). For online RL, it’s single data entry (since we are constantly getting new data through greedy dearch) or mini-batch. We are doing an offline RL task, thus batch RL.

Several obstacles were encountered after we altered the DRQN into an offline version (the only difference is that we have a fixed pool for experience replay, while in the online version there are always new data coming into the pool). First, the model starts diverging before converging to the minimum loss. Second, though the model can achive its best performance (which is significantly higher than a random policy) when the loss is at its minimum, it cannot achieve a comparable performance with DKT-expectimax. Currently, the experiments of DKT are only restricted to the BKT simulated data (where we have the ground truth BKT). Table \ref{tb:planning} shows the comparison of different models and settings.

\begin{table}[]
\centering
\caption{Planning performances comparison. BKT and DKT models are used in a 5-step expectimax search for planning. The number of training instances is also listed. BKT-expectimax, with the BKT being the ground truth model, is the perfect policy. Random policy randomly picks a question at each time step, and serves as a baseline. (The 0.8 to 1.0 standard deviations are omitted in the table.)}
\label{tb:planning}
\begin{tabular}{|c|c|}
\hline
Algorithm       & Average rewards after a 20 problem episode \\ \hline
BKT             & 2.6                                        \\ \hline
DRQN \#ins=5000 & 2.1                                        \\ \hline
DKT \#ins=5000  & 2.6                                        \\ \hline
DKT \#ins=100   & 2.28                                       \\ \hline
Random          & 0.94                                       \\ \hline
\end{tabular}
\end{table}

As we have shown in section \ref{sec:DKT}, DKT can fully recover the BKT, so we are not surprised to see that a fully-trained DKT (with 5000 instances) can gain an optimal policy. However, in real world, we shouldn't expect DKT to fully recover the underlying model of real students. To better simulate this situation, we train DKT with little data (merely 100 instances). With this DKT, both the AUC on test set and the performance of its policy suffer a huge drop (which is consistent with our prior hypothesis, that DKT's performance is vulnerable to data available). 

However, even an inaccurate DKT like that still outperforms DRQN, and in this case DRQN is trained on way more data. This greatly conflicts our prior hpothesis, since DRQN should be more sample efficient and unless the myopic expectimax which can only look 5 steps ahead (when keeping the computation feasible), DRQN's Q-value estimation is infinite horizon (i.e. looks infinite steps ahead). 

We've tried several tricks and fine tuning for DRQN. For example, for offline training, the data distribution is fixed, so maybe we don’t need a target network like the online DQN. However, emperically, the trickes we have tried don’t make a difference.

The breakthrough finding came when we modify the algorithm to online (which is only possible for simulation since we need the ground truth simulator) it can perform on par with the optimal policy. When we do greedy search, it converges fast (because we have a lot of near-optimal sequences), while when we do completely random search, the performance drops after a short boost (just like the offline performance), then come back to the optimal performance eventually, which could be mainly due to the massive (around $10^5$ episodes) random dataset (in which there are enough good/near-optimal examples) that has already added in the pool. Greedy search is basically changing the training dataset, so we can easily accept the fact that it’s way better.

The current drawback of offline DRQN is clear: unable to do exploration, thus require a massive amount ($10^5$ episodes) of random data to see enough near-optimal trajectories. On the contrary, DKT can easily pick up a vague prerequisite structure even under a small dataset (100 episodes), by observing that some skills are easier to learn in the begining while others are not. Based on this little knowledge, the expectimax can get us an optimal policy. In this current setting, model-based method seems to win on planning performance (especially when model-free method is not that sample efficient as we thought it should be).

\subsection{Future Work}

Experimantal results so far have shown that deep batch Q-learning possibly couldn't work due to the inability of doing exploration in the environment. It turns out that the generalization ability of deep neural network is not that powerful as we expected. However, we are still trying to fine tune the DRQN to see if its performance could go up. A more promising direction is to integrate return based off-policy RL algorithms \footnote{\textit{Safe and Efficient Off-Policy Reinforcement Learning}, Remi Munos, Tom Stepleton, Anna Harutyunyan, Marc G. Bellemare, 2016.} into deep RL. This has not been done by any previous work before thus we might be on to something really innovative.

There are several concerns on the current experiment setting on simulation though. We only have 20 trials for each episode, i.e. our horizon is fairly short. A method as shallow as DKT-expectimax (5 steps look ahead) can perform as near-perfect because we can only expect the students to learn those skills without prerequisites. If we have a setting where a shallow sight might be catastrphic (like a longer horizon and a more complicated skill structure, with loose prerequisite relations as in the real world), we might see DRQN to win, since it can look infinite steps. However, none of these could account for the low performance of DRQN on the simpler simulation data (it should be able to get the optimal policy).

A minor technical issue we need to point out here. First, currently the model-based method is technically not a real RL algorithm, since the reward model is not learned but directly coded in the tree search algorithm. We can use something like DKT to learn the model, but in this case it would be a regression task rather than a classification one. The fix won't be easy and is not going to help with our main concern (why DRQN is underperforming) either.

There are more interesting directions that we have in mind, including planning with multi-skill questions and making sense of the necessary horizon length for getting an optimal policy (in extreme cases like all the skills are independent, pick any skill wouldn't make a difference, thus zero horizon is as good as infinite horizon). As for application, we should also account for a certain dropout rate when searching for policy. If a student drops out in the middle, a short horizon might be better, since the long-term benefits we planned for the student might not pay off. These thoughts are even more off-topic, and could be discussed in future projects.